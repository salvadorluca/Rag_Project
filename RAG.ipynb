{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook you can ask the RAG model the question you want, keep in mind the documents that we used were financial news from the first quarter of 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First set the working directory to scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"scripts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Embed the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can embed the data, this process may take a long time and we strongly advise to download the already embedded data from [here](https://epflch-my.sharepoint.com/:f:/g/personal/marco_giuliano_epfl_ch/EomT1E_2ZaFCkB7zpx-jDAMBxLz6UP8NGtdBNvK10RaYHQ?e=w8s5QY).\\\n",
    "If you want to embed the data yourself we advise you to use a GPU, otherwis it can take some days!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embed import embed\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"ashraq/financial-news-articles\")\n",
    "print(\"Data downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed wtih BGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"BGE\"\n",
    "path = \"../embeddings/save\"\n",
    "\n",
    "embed(model, path, ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed with BGE_Finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"BGE_Finetuned\"\n",
    "path = \"../embeddings/save_finetuned\"\n",
    "\n",
    "embed(model, path, ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the embedded data in the \"embeddings\" folder you can start retrieving the data and generating responses based on a query.\\\n",
    "If you want to try to use the BGE pretrained model make sure you have the embeddings saved in embeddings/save.\\\n",
    "If you want to try to use the Finetuned BGE  model make sure you have the embeddings saved in embeddings/save_finetuned and the model weights saved in models/BGE_Finetuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resource module not available on Windows\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from utils import choose_embed_model\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.schema import TextNode\n",
    "from models import retrieve, answer\n",
    "from utils import data_to_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use reranking you need to set a cohere api key, not necessay otherwise (But still run the cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohere_api_key = \"COHERE_API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The openai key is necessary to generate the response from the LLM, please put yours here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai_api_key = \"sk-\"\n",
    "openai.api_key = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the model here between: \\\n",
    "BM25 set \"BM25\" \\\n",
    "BGE set \"BGE\" \\\n",
    "Finetuned BGE set \"BGE_Finetuned\" \\\n",
    "Hybrid search with BGE and BM25 set \"Hybrid_BGE\" \\\n",
    "Hybrid search with Finetuned BGE and BM25 set \"Hybrid_Finetuned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"Hybrid_Finetuned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 10                  # Number of documents to retrieve\n",
    "alpha = 0.9                 # Alpha value for the Hybrid retrieval, 0.9 is the optimal value we found\n",
    "query_rewriting = False     # Whether to use query rewriting or not\n",
    "reranking = False           # Whether to use reranking or not\n",
    "top_k_before_reranking = 50 # Number of documents to retrieve before reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 3.3.1, however, your version is 3.3.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding loaded!\n"
     ]
    }
   ],
   "source": [
    "path = False\n",
    "\n",
    "if model == \"BGE_Finetuned\" or model == \"Hybrid_Finetuned\":\n",
    "    path = \"../embeddings/save_finetuned\"\n",
    "elif model == \"BGE\" or model == \"Hybrid_BGE\":\n",
    "    path = \"../embeddings/save\"\n",
    "\n",
    "index = None\n",
    "\n",
    "if path:\n",
    "    chroma_client = chromadb.PersistentClient(path=path)\n",
    "    chroma_collection = chroma_client.get_or_create_collection(\"mydocs\")\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "\n",
    "    embed_model = choose_embed_model(model)\n",
    "    index = VectorStoreIndex.from_vector_store(\n",
    "        vector_store, \n",
    "        storage_context=storage_context,\n",
    "        embed_model = embed_model,\n",
    "        show_progress=True,\n",
    "    )\n",
    "    print(\"Embedding loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the BM25 model we need the not embedded data, please download this data with this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_nodes = None\n",
    "if model == \"BM25\" or model == \"Hybrid_Finetuned\" or model == \"Hybrid_BGE\":\n",
    "    from datasets import load_dataset\n",
    "    ds = load_dataset(\"ashraq/financial-news-articles\")\n",
    "    raw_nodes = [TextNode(text=f\"Title: {row['title']}\\nContent: {row['text']}\") for row in ds['train']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can ask a question to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now modify the query to ask any question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What does the Federal Reserve say on interest rates?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_nodes = retrieve(model, \n",
    "                           query, \n",
    "                           index, \n",
    "                           raw_nodes, \n",
    "                           top_k, \n",
    "                           alpha=alpha, \n",
    "                           query_rewriting=query_rewriting, \n",
    "                           openai_api_key=openai_api_key, \n",
    "                           reranking=reranking, \n",
    "                           top_k_before_reranking=top_k_before_reranking, \n",
    "                           cohere_api_key=cohere_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = answer(query, retrieved_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Federal Reserve decided to keep interest rates unchanged at its recent meeting, but expressed confidence that a recent rise in inflation to near its 2% target would be sustained. This leaves the Federal Reserve on track to raise borrowing costs in the near future, with expectations of further gradual increases in the federal funds rate.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see which nodes were used by the LLM to answer the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_df([n.node for n in retrieved_nodes])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
